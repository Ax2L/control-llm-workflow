[TAIPY]

[CORE]
read_entity_retry = "1:int"
core_version = "3.0"

[DATA_NODE.prompt]
storage_type = "pickle"
scope = "SCENARIO:SCOPE"
default_data = "\"Hello World\""

[DATA_NODE.response_data]
storage_type = "pickle"
scope = "SCENARIO:SCOPE"

[DATA_NODE.model_name]
storage_type = "pickle"
scope = "SCENARIO:SCOPE"
default_data = "llama3.1"

[DATA_NODE.conversation_id]
storage_type = "pickle"
scope = "SCENARIO:SCOPE"
default_data = "123456"

[DATA_NODE.result_data]
storage_type = "pickle"
scope = "SCENARIO:SCOPE"

[DATA_NODE.path_or_data]
storage_type = "json"
scope = "SCENARIO:SCOPE"
default_data = "data/prompts/benchmarks/jokes.json"

[TASK.ollama_instruction]
function = "functions.send_instruction_to_ollama:function"
inputs = [ "prompt:SECTION", "model_name:SECTION", "conversation_id:SECTION",]
outputs = [ "response_data:SECTION",]
skippable = "False:bool"

[TASK.oobabooga_instruction]
function = "functions.send_instruction_to_oobabooga:function"
inputs = [ "prompt:SECTION", "conversation_id:SECTION",]
outputs = [ "response_data:SECTION",]
skippable = "False:bool"

[TASK.process_list]
function = "functions.llm_process_list_of_prompts:function"
inputs = [ "conversation_id:SECTION", "path_or_data:SECTION",]
outputs = [ "result_data:SECTION",]
skippable = "False:bool"

[SCENARIO.llm_ollama_instruction]
tasks = [ "ollama_instruction:SECTION",]
additional_data_nodes = []

[SCENARIO.llm_oobabooga_instruction]
tasks = [ "oobabooga_instruction:SECTION",]
additional_data_nodes = []

[SCENARIO.process_list_of_prompts]
tasks = [ "process_list:SECTION",]
additional_data_nodes = []

[SCENARIO.llm_ollama_instruction.comparators]

[SCENARIO.llm_ollama_instruction.sequences]

[SCENARIO.llm_oobabooga_instruction.comparators]

[SCENARIO.llm_oobabooga_instruction.sequences]

[SCENARIO.process_list_of_prompts.comparators]

[SCENARIO.process_list_of_prompts.sequences]
